---
title: "Assignment 1 - Group 8"
author: "Elia Di Gregorio and Robert Auerbach"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Quick note

Super cool trick for the R_Markdown is to have saved the workspace of your basic R_file somewhere in the project and then load it directly. In this way you have all the necessary objects and plots from that basic R_file, within your R_Markdown_file, too. There is no need to repeat and rerun the code to plot something. You can just "call" it :)

```{r}
source("./code/__packages.R")
load("./WS_CD.RData")
load("./WS_AB.RData")
```

## Exercise A
The dependent variable medv shows the median value of owner-occupied homes in $1000s.
We chose the following covariates for our linear model:
1) crim: per capita crime rate by town.
2) zn: proportion of residential land zoned for lots over 25,000 sq.ft.
3) indus: proportion of non-retail business acres per town.
4) chas: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).
5) nox: nitrogen oxides concentration (parts per 10 million).

Task: Create a function that takes your dependent variable and the covariates as inputs,
and return a list with:
– OLS point estimates for the intercept, slope parameters, and the error variance.
– Suitable test statistics with corresponding p-values for the relevant coefficients.
– Intervals of the coefficients for a confidence level of 95%.
```{r}
pp_pred <- function(dependent_variable, covariates) {
  data <- data.frame(dependent_variable, covariates)
  model <- lm(dependent_variable ~ ., data)
  coefficients <- coef(model)
  se <- summary(model)$coefficients[, "Std. Error"]
  t_values <- coefficients / se
  p_values <- 2 * pt(abs(t_values), df = df.residual(model), lower.tail = FALSE)
  conf_int <- confint.default(model)
  results <- data.frame(
    "Coefficients" = coefficients,
    "Standard Errors" = se,
    "t-values" = t_values,
    "p-values" = p_values,
    "Confidence Intervals (95%)" = conf_int
  )
  
  return(results)
}
```

# Call the function and print list
```{r}
result <- property_price_prediction(dependent_variable, covariates)
print(result)
```

## Exercise B

Task: Come up with some network of interest, vaguely related to some real-world example
(describe very briefly), with at least six agents and ten edges between them.

Our real world example is one of a supply chain network where there is rarely any reciprocity: construction of airplanes. The main vertices is the firm constructing the airplane (B), where all manufactured parts end up (high in-degree, low out-degree). On the other hand, among manufactures of airplane parts little reciprocity might occur as they need specialized part to finish their own parts (D and A).

B.1: Draw a graph of the network; create the adjacency matrix in R.
```{r}
plot(g, edge.arrow.size = 0.5, vertex.label.cex = 1.5, vertex.size = 30)

print(adj_matrix)
```

B.2: Who are the most and least central agents in the network? Name, explain, and try
to quantify different notions of centrality.

We quantified the in- and out-degree of centrality. The in-degree shows how many links are directed towards are node, while the out-degree shows how many links are directed from a node (to another). A node from which a link goes out to another is considered a supplier, a node which is receiving links is considered a buyer.
The degree_table shows how many links go in and out from each node. This way we can quantify the most central buyer and supplier in our network.
According to the table B and C have the most links directed towards them while E has none directed towards it. Hence, B and C are the most central "buyers", E the least.
Also, A and C have the most links directed towards others while B has none directed towards others. Hence A and C are the most central "suppliers" and B the least.

```{r}
degree_table
```

B.2.1: How would centrality change if you considered a row-normalized network instead?

We calculate the sums of each row of the adjacency matrix and then divide the addjacency matrix by the row sums.
Again, we compute the centrality measures of in- and out-degree. As expected (based on the slides) the out-degree of the agents are equalized to 1. Also, the most central buyers are now B and D instead of B and C. Clearly the row-normalization leads to a distortion.
```{r}
degree_table_norm
```

B.2.2: How would the network change if you removed or added a specific agent?
We removed agent "F" and repeated the steps from above.
```{r}
plot(g1, edge.arrow.size = 0.5, vertex.label.cex = 1.5, vertex.size = 30)
reciprocity(g1)
transitivity(g1)
```



## Exercise C

In this exercise, we will work with spatial projections of Türkiye's NUTS-2 regions. Despite not being a member of the European Union, Türkiye has adopted the Nomenclature of Territorial Units for Statistics (NUTS) since 2002. The country counts 26 subregions, and we accessed its spatial data directly from the [GISCO](https://ec.europa.eu/eurostat/web/gisco/geodata/reference-data/administrative-units-statistical-units/nuts) source via `get_eurostat_spatial()` function. By default this function loads the EPSG-4326 projection of the map, corresponding to the World Geodetic System 1984 ensemble (WGS84). It is based on a geocentric datum, meaning it defines the Earth's shape as an ellipsoid (a flattened sphere) rather than a perfect sphere.

```{r}
st_crs(türkiye)
```
To use another projection and/or CRS we employed the `st_transform()` function and recurred to the Equal Earth Projection and to the Lambert Azimuthal Equal Area Projection. Both preserve the relative sizes of areas on the Earth's surface. This means that areas on the map are represented accurately in relation to each other in terms of size, making it suitable for thematic mapping and spatial analysis. However the former is classified as a pseudo-cylindrical projection because it projects the Earth's surface onto a cylinder and then unwraps the cylinder to form a rectangular map, whereas the latter projects the Earth's surface onto a plane tangent to a specific point (the center of the projection).
The difference in projections is highlighted by the following map:

```{r}
plot(combined_plot)
```
For the second part of the Exercise we decided to use the dataset `tgs00111`: Nights spent at tourist accommodation establishments by NUTS 2 regions ([here](<https://ec.europa.eu/eurostat/cache/metadata/en/tour_occ_esms.htm) the Metadata for consultation). The dataset covers internal tourism, in other words tourism flows within the country (domestic tourism) or from abroad to destinations in the country (inbound tourism) for the year 2022. For the sake of the exercise we further enhanced the dataset with two additional columns with the share of domestic and foreign tourist overnights (continuous scale), and one column for a factor variable based on the condition that if the domestic tourism share of overnight stay is greater than 50%, the tourist is labeled as "Domestic Tourist"; otherwise, they are labeled as "Foreign Tourist".

By plotting the latter variable, we were able to group together regions based on whether the majority of overnight stay in hotels were by domestic tourists or foreign tourists.

```{r}
plot(TR_plot_1)
```
Not surprisingly, regions with traditional "instagrammable" destinations like Istanbul or Antalya and Burdur had a majority of foreign tourists staying overnight in hotels, while the rest of the country registered a higher domestic tourists inflow, suggesting a very limited international touristic network in Türkiye.

The plot for the continuous scale variable focused more on the domestic dimension of tourism and represented the share of domestic tourist overnight stays in different Turkish regions.

```{r}
plot(TR_plot_2)
```
Besides stressing the point made by the previous map, this time we get more insights on the distribution of Domestic tourists in Türkiye. To further enrich the plot, we also decided to highlight the names of those 10 regions with beyond 90% of domestic tourists overnight stays.

Concerning the last question of the exercise, there are two conceptually different ways to store visualizations: raster-based and vector-based formats. 

Raster-based formats like PNG and JPEG store images as grids of pixels, making them suitable for complex color gradients and detailed images. However, they are resolution-dependent, which means they can lose quality when scaled up.

On the other hand, vector-based formats such as SVG and PDF store image data using mathematical formulas to define shapes, lines, and colors. This makes them ideal for visualizations with geometric shapes, charts, maps, and illustrations where scalability and high-quality printing are crucial. Unlike raster formats, vector graphics are resolution-independent and can be scaled without loss of quality.

For visualizations created using R and ggplot2, which inherently produce vector graphics, it is recommended to save them in vector-based formats like SVG or PDF. These formats maintain sharpness and clarity when scaled to any size, making them suitable for presentations, printing, and high-resolution displays. SVG is particularly useful for web-based graphics and interactive visualizations, while PDF is excellent for high-quality printing and cross-platform compatibility.

```{r}
# Save the plot as SVG
#ggsave("TR_plot_2.svg", plot = TR_plot_2, device = "svg")
```

## Exercise D
